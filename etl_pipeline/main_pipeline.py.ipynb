import os
import glob
import time
import requests
import urllib3
import duckdb

# =============================================================================
# CONFIGURATION
# =============================================================================
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

BASE_DIR = os.path.dirname(os.path.abspath(__file__))
DATA_DIR = os.path.join(BASE_DIR, '..', 'data_raw')
OUTPUT_DIR = os.path.join(BASE_DIR, '..', 'data_processed')

os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

YEAR = 2024
MONTHS = range(1, 13)

# =============================================================================
# STEP 1: INGESTION (BTS Download)
# =============================================================================
def download_flight_data():
    """Downloads monthly ZIPs from BTS with retry logic."""
    base_url = "https://transtats.bts.gov/PREZIP/On_Time_Reporting_Carrier_On_Time_Performance_1987_present_{}_{}.zip"
    print(f"\n--- [STEP 1] Starting Ingestion for {YEAR} ---")
    
    for month in MONTHS:
        filename = f"flights_{YEAR}_{month}.zip"
        file_path = os.path.join(DATA_DIR, filename)
        url = base_url.format(YEAR, month)
        
        if os.path.exists(file_path):
            print(f"Skipping {month}/{YEAR} (Exists)")
            continue

        print(f"Downloading {month}/{YEAR}...", end=" ")
        for attempt in range(3):
            try:
                time.sleep(3)
                r = requests.get(url, verify=False, stream=True, timeout=60)
                if r.status_code == 200:
                    with open(file_path, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=1024):
                            f.write(chunk)
                    print("Success.")
                    break
            except Exception:
                time.sleep(5)
                if attempt == 2: print("Failed.")

# =============================================================================
# STEP 2: DUCKDB ETL & FINANCIAL MODELING
# =============================================================================
def run_duckdb_pipeline():
    """
    Utilizes DuckDB for out-of-core processing.
    Extracts ZIP/CSV data, builds a Star Schema, fixes liability logic, 
    and exports natively to Parquet.
    """
    print("\n--- [STEP 2] Initializing DuckDB Pipeline ---")
    
    # Connect to an in-memory DuckDB database
    con = duckdb.connect(':memory:')
    
    # Check if raw files exist
    csv_files = glob.glob(os.path.join(DATA_DIR, "*.csv"))
    # If only ZIPs exist, DuckDB can't read them natively without extensions, 
    # so ensure you have extracted the CSVs to DATA_DIR prior, or modify to read extracted files.
    
    print("Executing Vectorized Transformations & Exporting Fact Table...")
    
    # Create the Fact Table directly from CSVs to Parquet
    con.execute(f"""
        COPY (
            SELECT 
                -- 1. Upstream Surrogate Key Generation
                uuid() AS Flight_ID,
                
                -- 2. Core Flight Metrics
                CAST(FlightDate AS DATE) AS FlightDate,
                Tail_Number,
                Origin,
                Dest,
                Reporting_Airline,
                TRY_CAST(DepDelay AS DOUBLE) AS DepDelay,
                TRY_CAST(ArrDelay AS DOUBLE) AS ArrDelay,
                TRY_CAST(AirTime AS DOUBLE) AS AirTime,
                TRY_CAST(Distance AS DOUBLE) AS Distance,
                TRY_CAST(Cancelled AS INTEGER) AS Cancelled,
                TRY_CAST(Diverted AS INTEGER) AS Diverted,
                
                -- 3. Financial Logic: Estimated Delay Cost ($75/min)
                CASE 
                    WHEN TRY_CAST(ArrDelay AS DOUBLE) > 0 THEN TRY_CAST(ArrDelay AS DOUBLE) * 75.0
                    ELSE 0.0 
                END AS Estimated_Cost,
                
                -- 4. Financial Logic: Maintenance Liability (CRITICAL BUG FIX)
                CASE 
                    WHEN TRY_CAST(Cancelled AS INTEGER) = 1 OR TRY_CAST(AirTime AS DOUBLE) IS NULL THEN 0.0
                    ELSE ((TRY_CAST(AirTime AS DOUBLE) / 60.0) * 250.0) + 180.0
                END AS MRL_Liability,
                
                -- 5. Delay Root Causes (Zero-Fill NULLs)
                COALESCE(TRY_CAST(CarrierDelay AS DOUBLE), 0.0) AS CarrierDelay,
                COALESCE(TRY_CAST(WeatherDelay AS DOUBLE), 0.0) AS WeatherDelay,
                COALESCE(TRY_CAST(NASDelay AS DOUBLE), 0.0) AS NASDelay,
                COALESCE(TRY_CAST(SecurityDelay AS DOUBLE), 0.0) AS SecurityDelay,
                COALESCE(TRY_CAST(LateAircraftDelay AS DOUBLE), 0.0) AS LateAircraftDelay
                
            FROM read_csv_auto('{DATA_DIR}/*.csv', filename=true, all_varchar=true)
        ) TO '{OUTPUT_DIR}/Aviation_Fact_Table.parquet' (FORMAT PARQUET, CODEC 'SNAPPY');
    """)
    print("Fact Table Exported: Aviation_Fact_Table.parquet")

    # Extract Dimension: Airlines
    print("Exporting Airline Dimension...")
    con.execute(f"""
        COPY (
            SELECT DISTINCT Reporting_Airline, DOT_ID_Reporting_Airline, IATA_CODE_Reporting_Airline
            FROM read_csv_auto('{DATA_DIR}/*.csv', filename=true, all_varchar=true)
            WHERE Reporting_Airline IS NOT NULL
        ) TO '{OUTPUT_DIR}/Aviation_Airline_Dim.parquet' (FORMAT PARQUET, CODEC 'SNAPPY');
    """)

    # Extract Dimension: Geography (Origin & Dest combined)
    print("Exporting Geography Dimension...")
    con.execute(f"""
        COPY (
            SELECT DISTINCT Origin AS AirportCode, OriginCityName AS City, OriginStateName AS State
            FROM read_csv_auto('{DATA_DIR}/*.csv', filename=true, all_varchar=true)
            UNION
            SELECT DISTINCT Dest AS AirportCode, DestCityName AS City, DestStateName AS State
            FROM read_csv_auto('{DATA_DIR}/*.csv', filename=true, all_varchar=true)
        ) TO '{OUTPUT_DIR}/Aviation_Geo_Dim.parquet' (FORMAT PARQUET, CODEC 'SNAPPY');
    """)

    print("SUCCESS: DuckDB Pipeline Complete. Dimensions and Fact tables ready.")

# =============================================================================
# MAIN
# =============================================================================
if __name__ == "__main__":
    # Ensure your CSV files are extracted from the ZIPs into the DATA_DIR 
    # before running the DuckDB pipeline.
    
    # download_flight_data() 
    run_duckdb_pipeline()
